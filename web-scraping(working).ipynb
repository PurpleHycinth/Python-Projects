{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1669887883114,"user":{"displayName":"SWARALI DESHMUKH 21112028","userId":"12817005262726704804"},"user_tz":-330},"id":"aKGq-yMKEj0N","outputId":"df004947-e6a9-4644-d0a3-0285de337162"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting snscrape\n","  Downloading snscrape-0.4.3.20220106-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.6.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from snscrape) (3.8.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snscrape) (2022.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.9.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from snscrape) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2022.9.24)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.7.1)\n","Installing collected packages: snscrape\n","Successfully installed snscrape-0.4.3.20220106\n"]}],"source":["!pip install snscrape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1669888108434,"user":{"displayName":"SWARALI DESHMUKH 21112028","userId":"12817005262726704804"},"user_tz":-330},"id":"gfxx3bIoExvX","outputId":"856c4538-ae64-4840-8821-3ec6b459fa32"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'url': 'https://twitter.com/OlgaSantarovich/status/1598252506261364739', 'date': datetime.datetime(2022, 12, 1, 9, 47, 46, tzinfo=datetime.timezone.utc), 'content': 'The Complete #Pandas Bootcamp 2023 for Data Analysis – #Python https://t.co/di8cUQPuvY', 'renderedContent': 'The Complete #Pandas Bootcamp 2023 for Data Analysis – #Python coursejoiner.com/free-udemy/the…', 'id': 1598252506261364739, 'user': User(username='OlgaSantarovich', id=15872246, displayname='Olga Santarovich', description=\"Live in the future, then build what's missing.\", rawDescription=\"Live in the future, then build what's missing.\", descriptionUrls=None, verified=False, created=datetime.datetime(2008, 8, 16, 9, 12, 21, tzinfo=datetime.timezone.utc), followersCount=364, friendsCount=319, statusesCount=4654, favouritesCount=4132, listedCount=18, mediaCount=711, location='', protected=False, linkUrl=None, linkTcourl=None, profileImageUrl='https://pbs.twimg.com/profile_images/1523716994427768837/t527GH8E_normal.jpg', profileBannerUrl='https://pbs.twimg.com/profile_banners/15872246/1576248175', label=None), 'replyCount': 0, 'retweetCount': 0, 'likeCount': 0, 'quoteCount': 0, 'conversationId': 1598252506261364739, 'lang': 'en', 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'sourceUrl': 'http://twitter.com/download/iphone', 'sourceLabel': 'Twitter for iPhone', 'outlinks': ['https://www.coursejoiner.com/free-udemy/the-complete-pandas-bootcamp-2023-for-data-analysis-python/'], 'tcooutlinks': ['https://t.co/di8cUQPuvY'], 'media': None, 'retweetedTweet': None, 'quotedTweet': None, 'inReplyToTweetId': None, 'inReplyToUser': None, 'mentionedUsers': None, 'coordinates': None, 'place': None, 'hashtags': ['Pandas', 'Python'], 'cashtags': None}\n"]}],"source":["import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","query= 'python'\n","for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n","  print(vars(tweet))\n","  break"]},{"cell_type":"code","source":["import os\n","import json\n","import time\n","from itertools import chain\n","import snscrape.modules.twitter as sntwitter\n","import tweepy\n","from tweepy import RateLimitError\n","\n","from utils import save_to_csv, merge_txt_files_scraped, ROOT_DIR, SCRAPED_TWEET_PATH\n","\n","# get twitter app data saved in a json file\n","twitter_auth_data = open(\"twitter_auth_data.json\").read()\n","twitter_auth_data_json = json.loads(twitter_auth_data)\n","\n","access_token = twitter_auth_data_json[\"access_token\"]\n","access_token_secret = twitter_auth_data_json[\"access_token_secret\"]\n","consumer_key = twitter_auth_data_json[\"consumer_key\"]\n","consumer_secret = twitter_auth_data_json[\"consumer_secret\"]\n","\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_token_secret)\n","api = tweepy.API(auth)\n","\n","\n","def snscrape_ids(keyword_user_search_param, keywords_users_list, since, until, lang):\n","    dir_name = f\"{since.replace('-', '')}_{until.replace('-', '')}\"\n","\n","    try:\n","        os.mkdir(\"scraped_tweet\")\n","    except FileExistsError:\n","        print(\"Directory 'scraped_tweet' already exists\")\n","\n","    try:\n","        os.chdir(SCRAPED_TWEET_PATH)\n","        os.mkdir(dir_name)\n","        print(\"Directory\", dir_name, \"Created \")\n","    except FileExistsError:\n","        print(\"Directory\", dir_name, \"already exists\")\n","\n","    for keyword in keywords_users_list:\n","        if len(keyword) > 0:\n","            output_name = f\"{keyword.replace(' ', '_')}_{since.replace('-', '')}_{until.replace('-', '')}.txt\"\n","            output_path = os.path.join(ROOT_DIR, 'scraped_tweet', dir_name, output_name)\n","\n","            print(f'scraping tweets with keyword: \"{keyword}\" ...')\n","            try:\n","                os.system(\n","                    f'snscrape twitter-{keyword_user_search_param} \"{keyword} since:{since} until:{until} lang:{lang}\" > {output_path}')\n","            except Exception as err:\n","                print(f\"SNSCRAPE ERROR: {err}\")\n","\n","    print(f'Scraped all tweets in keywords list.')\n","\n","    # merge all txt files in a folder in a single txt file and delete duplicated ids\n","    joined_txt_no_duplicate = merge_txt_files_scraped(dir_name)\n","\n","    with open(f\"tweets_ids_{dir_name}.txt\", \"w\") as outfile:\n","        outfile.writelines(joined_txt_no_duplicate)\n","        print(f\"'tweets_ids_{dir_name}.txt' saved in folder {dir_name}\")\n","\n","    return joined_txt_no_duplicate\n","\n","\n","# send request to twitter using tweepy (input: batch of 50 ids, output: for each ids a tweet containing:\n","# {id, username, text, date, location, keyword} )\n","def twitter_api_caller(keyword_user_search_param, keywords_list, ids, batch_size, save_dir, csv_name, collect_replies):\n","    if keyword_user_search_param == 'search':\n","        csv_columns = ['id', 'username', 'text', 'keywords', 'date', 'location']\n","    else:\n","        csv_columns = ['id', 'username', 'text', 'date', 'location']\n","\n","    if collect_replies:\n","        csv_columns.append('replies')\n","\n","    try:\n","        os.chdir(SCRAPED_TWEET_PATH)\n","        os.mkdir(save_dir)\n","\n","        print(\"Directory 'final_tweet_csv' Created\")\n","    except FileExistsError:\n","        print(\"Directory 'final_tweet_csv' already exists\")\n","\n","    n_chunks = int((len(ids) - 1) // batch_size + 1)\n","\n","    tweets = []\n","    i = 0\n","    while i < n_chunks:\n","\n","        if i > 0 and i % 300 == 0:\n","            # if batch number exceed 300 request could fail\n","            time.sleep(60)\n","\n","        if i != n_chunks - 1:\n","            batch = ids[i * batch_size:(i + 1) * batch_size]\n","        else:\n","            batch = ids[i * batch_size:]\n","\n","        print(f\"Processing batch n° {i + 1}/{n_chunks} ...\")\n","        try:\n","            list_of_tw_status = api.statuses_lookup(batch, tweet_mode=\"extended\")\n","        except RateLimitError as err:\n","            print('Tweepy: Rate Limit exceeded')\n","            # https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/faq\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            break\n","        except Exception as err:\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            print(f\"General Error: {str(err)}\")\n","            break\n","\n","        tweets_batch = []\n","        for status in list_of_tw_status:\n","            try:\n","                tweet = {\"id\": status.id,\n","                         \"username\": status.user.screen_name,\n","                         \"text\": status.full_text.replace('\\n', ' '),\n","                         \"date\": str(status.created_at),\n","                         \"location\": status.user.location}\n","\n","                if keyword_user_search_param == 'search':\n","                    keywords_in_tweet = get_tweet_keywords(keywords_list, status)\n","\n","                    tweet[\"keywords\"] = list(set(keywords_in_tweet))\n","\n","                if collect_replies:\n","                    replies = collect_tweet_replies(status.id, max_num_replies=100)\n","                    tweet['replies'] = replies\n","\n","            except Exception as err:\n","                print(f\"General Error: {str(err)}\")\n","                continue\n","            tweets_batch.append(tweet)\n","        print(f\"Processed - scraped {len(tweets_batch)} tweets.\")\n","        if len(tweets_batch) == 0:\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            print(\"No tweets scraped\")\n","            break\n","\n","        i += 1\n","        tweets.append(tweets_batch)\n","\n","    save_to_csv(tweets, save_dir, csv_name, csv_columns)\n","\n","\n","def get_tweet_keywords(keywords_list, status):\n","    keywords_tweet = []\n","    for keyword in keywords_list:\n","        if len(keyword.split()) == 1:\n","            if keyword.lower() in status.full_text.lower() or keyword.lower() in status.user.screen_name.lower():\n","                keywords_tweet.append(keyword)\n","        elif len(keyword.split()) > 1:\n","            boolean_parameter = []\n","            for word in keyword.split():\n","                is_present = word.lower() in status.full_text.lower()\n","                boolean_parameter.append(is_present)\n","            if all(boolean_parameter):\n","                keywords_tweet.append(keyword)\n","        else:\n","            continue\n","    return list(set(keywords_tweet))\n","\n","\n","def collect_tweet_replies(tweet_id, max_num_replies):\n","    replies_ids = []\n","\n","    for reply in sntwitter.TwitterSearchScraper(\n","            query=f\"conversation_id:{tweet_id} (filter:safe OR -filter:safe)\").get_items():\n","        replies_ids.append(reply.id)\n","\n","    batch_size_replies = 50\n","    n_chunks_repl = int((len(replies_ids) - 1) // batch_size_replies + 1)\n","\n","    replies = []\n","    i = 0\n","    while i < n_chunks_repl:\n","\n","        if i > 0 and i % 300 == 0:\n","            # if batch number exceed 300 request could fail\n","            time.sleep(60)\n","\n","        if max_num_replies <= i*batch_size_replies:\n","            # if too many replies\n","            break\n","\n","        if i != n_chunks_repl - 1:\n","            batch = replies_ids[i * batch_size_replies:(i + 1) * batch_size_replies]\n","        else:\n","            batch = replies_ids[i * batch_size_replies:]\n","\n","        print(f\"Processing REPLIES batch n° {i + 1}/{n_chunks_repl} ...\")\n","        try:\n","            list_of_tw_status_reply = api.statuses_lookup(batch, tweet_mode=\"extended\")\n","        except:\n","            break\n","\n","        replies_batch = []\n","        for status_reply in list_of_tw_status_reply:\n","            # print(status_reply)\n","            if hasattr(status_reply, 'full_text'):\n","                reply = {\n","                    \"id\": status_reply.id,\n","                    \"username\": status_reply.user.screen_name,\n","                    \"text\": status_reply.full_text.replace('\\n', ' ')\n","                }\n","                # print(reply)\n","                replies_batch.append(reply)\n","        i += 1\n","        replies.append(replies_batch)\n","\n","    return list(chain.from_iterable(replies))\n","\n","\n","def fetch_tweets(keyword_user_search_param, keywords_users_list, since, until, lang, batch_size, save_dir, csv_name,\n","                 collect_replies):\n","    users_and_ids = snscrape_ids(keyword_user_search_param, keywords_users_list, since, until, lang)\n","    ids = list(map(lambda x: x.split(\" \")[1].strip('\\n'), users_and_ids))\n","    twitter_api_caller(keyword_user_search_param, keywords_users_list, ids, batch_size, save_dir, csv_name,\n","                       collect_replies)"],"metadata":{"id":"11UPsoMOMCYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import time\n","from itertools import chain\n","import snscrape.modules.twitter as sntwitter\n","import tweepy\n","from tweepy import RateLimitError\n","\n","from utils import save_to_csv, merge_txt_files_scraped, ROOT_DIR, SCRAPED_TWEET_PATH\n","\n","# get twitter app data saved in a json file\n","twitter_auth_data = open(\"twitter_auth_data.json\").read()\n","twitter_auth_data_json = json.loads(twitter_auth_data)\n","\n","access_token = twitter_auth_data_json[\"access_token\"]\n","access_token_secret = twitter_auth_data_json[\"access_token_secret\"]\n","consumer_key = twitter_auth_data_json[\"consumer_key\"]\n","consumer_secret = twitter_auth_data_json[\"consumer_secret\"]\n","\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_token_secret)\n","api = tweepy.API(auth)\n","\n","\n","def snscrape_ids(keyword_user_search_param, keywords_users_list, since, until, lang):\n","    dir_name = f\"{since.replace('-', '')}_{until.replace('-', '')}\"\n","\n","    try:\n","        os.mkdir(\"scraped_tweet\")\n","    except FileExistsError:\n","        print(\"Directory 'scraped_tweet' already exists\")\n","\n","    try:\n","        os.chdir(SCRAPED_TWEET_PATH)\n","        os.mkdir(dir_name)\n","        print(\"Directory\", dir_name, \"Created \")\n","    except FileExistsError:\n","        print(\"Directory\", dir_name, \"already exists\")\n","\n","    for keyword in keywords_users_list:\n","        if len(keyword) > 0:\n","            output_name = f\"{keyword.replace(' ', '_')}_{since.replace('-', '')}_{until.replace('-', '')}.txt\"\n","            output_path = os.path.join(ROOT_DIR, 'scraped_tweet', dir_name, output_name)\n","\n","            print(f'scraping tweets with keyword: \"{keyword}\" ...')\n","            try:\n","                os.system(\n","                    f'snscrape twitter-{keyword_user_search_param} \"{keyword} since:{since} until:{until} lang:{lang}\" > {output_path}')\n","            except Exception as err:\n","                print(f\"SNSCRAPE ERROR: {err}\")\n","\n","    print(f'Scraped all tweets in keywords list.')\n","\n","    # merge all txt files in a folder in a single txt file and delete duplicated ids\n","    joined_txt_no_duplicate = merge_txt_files_scraped(dir_name)\n","\n","    with open(f\"tweets_ids_{dir_name}.txt\", \"w\") as outfile:\n","        outfile.writelines(joined_txt_no_duplicate)\n","        print(f\"'tweets_ids_{dir_name}.txt' saved in folder {dir_name}\")\n","\n","    return joined_txt_no_duplicate\n","\n","\n","# send request to twitter using tweepy (input: batch of 50 ids, output: for each ids a tweet containing:\n","# {id, username, text, date, location, keyword} )\n","def twitter_api_caller(keyword_user_search_param, keywords_list, ids, batch_size, save_dir, csv_name, collect_replies):\n","    if keyword_user_search_param == 'search':\n","        csv_columns = ['id', 'username', 'text', 'keywords', 'date', 'location']\n","    else:\n","        csv_columns = ['id', 'username', 'text', 'date', 'location']\n","\n","    if collect_replies:\n","        csv_columns.append('replies')\n","\n","    try:\n","        os.chdir(SCRAPED_TWEET_PATH)\n","        os.mkdir(save_dir)\n","\n","        print(\"Directory 'final_tweet_csv' Created\")\n","    except FileExistsError:\n","        print(\"Directory 'final_tweet_csv' already exists\")\n","\n","    n_chunks = int((len(ids) - 1) // batch_size + 1)\n","\n","    tweets = []\n","    i = 0\n","    while i < n_chunks:\n","\n","        if i > 0 and i % 300 == 0:\n","            # if batch number exceed 300 request could fail\n","            time.sleep(60)\n","\n","        if i != n_chunks - 1:\n","            batch = ids[i * batch_size:(i + 1) * batch_size]\n","        else:\n","            batch = ids[i * batch_size:]\n","\n","        print(f\"Processing batch n° {i + 1}/{n_chunks} ...\")\n","        try:\n","            list_of_tw_status = api.statuses_lookup(batch, tweet_mode=\"extended\")\n","        except RateLimitError as err:\n","            print('Tweepy: Rate Limit exceeded')\n","            # https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/faq\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            break\n","        except Exception as err:\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            print(f\"General Error: {str(err)}\")\n","            break\n","\n","        tweets_batch = []\n","        for status in list_of_tw_status:\n","            try:\n","                tweet = {\"id\": status.id,\n","                         \"username\": status.user.screen_name,\n","                         \"text\": status.full_text.replace('\\n', ' '),\n","                         \"date\": str(status.created_at),\n","                         \"location\": status.user.location}\n","\n","                if keyword_user_search_param == 'search':\n","                    keywords_in_tweet = get_tweet_keywords(keywords_list, status)\n","\n","                    tweet[\"keywords\"] = list(set(keywords_in_tweet))\n","\n","                if collect_replies:\n","                    replies = collect_tweet_replies(status.id, max_num_replies=100)\n","                    tweet['replies'] = replies\n","\n","            except Exception as err:\n","                print(f\"General Error: {str(err)}\")\n","                continue\n","            tweets_batch.append(tweet)\n","        print(f\"Processed - scraped {len(tweets_batch)} tweets.\")\n","        if len(tweets_batch) == 0:\n","            save_to_csv(tweets, save_dir, f\"{csv_name}_last_batch_{i}\", csv_columns)\n","            print(\"No tweets scraped\")\n","            break\n","\n","        i += 1\n","        tweets.append(tweets_batch)\n","\n","    save_to_csv(tweets, save_dir, csv_name, csv_columns)\n","\n","\n","def get_tweet_keywords(keywords_list, status):\n","    keywords_tweet = []\n","    for keyword in keywords_list:\n","        if len(keyword.split()) == 1:\n","            if keyword.lower() in status.full_text.lower() or keyword.lower() in status.user.screen_name.lower():\n","                keywords_tweet.append(keyword)\n","        elif len(keyword.split()) > 1:\n","            boolean_parameter = []\n","            for word in keyword.split():\n","                is_present = word.lower() in status.full_text.lower()\n","                boolean_parameter.append(is_present)\n","            if all(boolean_parameter):\n","                keywords_tweet.append(keyword)\n","        else:\n","            continue\n","    return list(set(keywords_tweet))\n","\n","\n","def collect_tweet_replies(tweet_id, max_num_replies):\n","    replies_ids = []\n","\n","    for reply in sntwitter.TwitterSearchScraper(\n","            query=f\"conversation_id:{tweet_id} (filter:safe OR -filter:safe)\").get_items():\n","        replies_ids.append(reply.id)\n","\n","    batch_size_replies = 50\n","    n_chunks_repl = int((len(replies_ids) - 1) // batch_size_replies + 1)\n","\n","    replies = []\n","    i = 0\n","    while i < n_chunks_repl:\n","\n","        if i > 0 and i % 300 == 0:\n","            # if batch number exceed 300 request could fail\n","            time.sleep(60)\n","\n","        if max_num_replies <= i*batch_size_replies:\n","            # if too many replies\n","            break\n","\n","        if i != n_chunks_repl - 1:\n","            batch = replies_ids[i * batch_size_replies:(i + 1) * batch_size_replies]\n","        else:\n","            batch = replies_ids[i * batch_size_replies:]\n","\n","        print(f\"Processing REPLIES batch n° {i + 1}/{n_chunks_repl} ...\")\n","        try:\n","            list_of_tw_status_reply = api.statuses_lookup(batch, tweet_mode=\"extended\")\n","        except:\n","            break\n","\n","        replies_batch = []\n","        for status_reply in list_of_tw_status_reply:\n","            # print(status_reply)\n","            if hasattr(status_reply, 'full_text'):\n","                reply = {\n","                    \"id\": status_reply.id,\n","                    \"username\": status_reply.user.screen_name,\n","                    \"text\": status_reply.full_text.replace('\\n', ' ')\n","                }\n","                # print(reply)\n","                replies_batch.append(reply)\n","        i += 1\n","        replies.append(replies_batch)\n","\n","    return list(chain.from_iterable(replies))\n","\n","\n","def fetch_tweets(keyword_user_search_param, keywords_users_list, since, until, lang, batch_size, save_dir, csv_name,\n","                 collect_replies):\n","    users_and_ids = snscrape_ids(keyword_user_search_param, keywords_users_list, since, until, lang)\n","    ids = list(map(lambda x: x.split(\" \")[1].strip('\\n'), users_and_ids))\n","    twitter_api_caller(keyword_user_search_param, keywords_users_list, ids, batch_size, save_dir, csv_name,\n","                       collect_replies)"],"metadata":{"id":"pRrD5t8OL3qA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Rcxfk5YuF4cd","outputId":"cb92cab0-033d-4f43-b3c5-079affb9bf64"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-4-d45d7bc9ba9f>:16: FutureWarning: username is deprecated, use user.username instead\n","  tweets.append([tweet.date, tweet.username, tweet.content])\n"]},{"name":"stdout","output_type":"stream","text":["                          Date      User  \\\n","0    2019-12-31 21:37:06+00:00  elonmusk   \n","1    2019-12-31 06:59:34+00:00  elonmusk   \n","2    2019-12-31 06:57:57+00:00  elonmusk   \n","3    2019-12-31 02:27:28+00:00  elonmusk   \n","4    2019-12-30 23:27:10+00:00  elonmusk   \n","...                        ...       ...   \n","4995 2018-01-28 00:42:40+00:00  elonmusk   \n","4996 2018-01-27 21:15:29+00:00  elonmusk   \n","4997 2018-01-24 18:35:00+00:00  elonmusk   \n","4998 2018-01-18 19:32:56+00:00  elonmusk   \n","4999 2018-01-15 04:07:05+00:00  elonmusk   \n","\n","                                                  Tweet  \n","0             @engineers_feed @physicsJ It’s a bit slow  \n","1                     @JohnnaCrider1 It’s not ready yet  \n","2                            @newscientist Explains 🐈 🎥  \n","3     @teslaownersSV @rhoehn Thanks all Tesla club m...  \n","4         Rest in peace Syd Mead. Your art will endure.  \n","...                                                 ...  \n","4995  Say hello to my little friend … https://t.co/O...  \n","4996  Aiming for first flight of Falcon Heavy on Feb...  \n","4997  Falcon Heavy hold-down firing this morning was...  \n","4998  @sallyshin Check https://t.co/Il3CWSTPXR for u...  \n","4999                               @Davidburrusdds Done  \n","\n","[5000 rows x 3 columns]\n"]}],"source":["import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","query = \"(from:elonmusk) until:2020-01-01 since:2010-01-01\"\n","tweets = []\n","limit = 5000\n","\n","\n","for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n","    \n","    # print(vars(tweet))\n","    # break\n","    if len(tweets) == limit:\n","        break\n","    else:\n","        tweets.append([tweet.date, tweet.username, tweet.content])\n","        \n","df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet'])\n","print(df)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONSCh78SOh94xzAwFB/EKG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}